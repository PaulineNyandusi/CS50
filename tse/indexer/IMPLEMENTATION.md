# CS50 Lab 3
## CS50 Fall 2023
## Pauline Nyandusi


### Indexer.c short explanation

The indexer.c program is designed to read document files generated by the TSE crawler, build an index mapping each word to its frequency in each document, and then write this index to a file. It utilizes a hashtable to efficiently manage the index and counters to track the frequency of each word.


### Usage

The indexer.c implements the following functions:


```c

static void indexBuild(const char *directory, const char *indexFileName);

static void indexPage(webpage_t* webpage, int docID, index_t* index);


```

### Implementation

Main Function

The main() function acts as the gateway to the program, coordinating various tasks and ensuring a smooth flow of operations. When the program is executed, it expects two command-line arguments: the name of the page directory and the index file name. The function first validates the number of arguments to ensure that both are provided. Subsequently, it checks the validity of the page directory and confirms whether the index file is writable. If all these conditions are met, it calls the indexBuild function to construct the index. The function returns an exit status, which is zero for a successful execution and non-zero for a failure.


The function indexBuild() serves as the cornerstone of the indexing process. It is tasked with building a comprehensive index based on the webpages stored in a specified directory. The function receives two string parameters, the directory path and the index file name. It starts by creating a new index object. Then, it goes through a loop where it reads each document ID, loads the corresponding webpage, and passes it along with the document ID to the indexPage function for indexing. It is crucial to note that the caller is responsible for ensuring that the directory exists and that the index file name is writable. Once the loop concludes.



The indexPage() function is at the heart of the indexing operation, focusing on a single webpage at a time. It is designed to receive a webpage object, its associated document ID, and the index where this information will be stored. The function iterates through each word in the webpage content, normalizes it to lowercase, and adds it to the index with its corresponding document ID. To ensure efficient memory usage, the function frees any dynamically allocated memory for each word after it has been processed. Therefore, this function not only indexes the webpage but also updates the index object, effectively contributing to the overall indexing mechanism.



### Files

* `Makefile` - compilation procedure
* `indexer.c` - the implementation
* `testing.sh` - The testing bash script
* `data` - test data
* `testing.out` - result of `make test &> testing.out`
* `indextest.c` - The index tester

### Compilation

To compile, simply `make `.

### Testing

Since i'm not sure if my output frommcrawler was accurate, i used the indexcmp program, which is specifically designed to compare two index files. The program is located at ~/cs50-dev/shared/tse/indexcmp. The indexcmp program expects two arguments: the paths to the index files you wish to compare

i have a script testing.sh that invokes the crawler several times, with a variety of command-line arguments. 

First, a sequence of invocations with  arguments, testing each of the possible mistakes that can be made. It also describes all of my tests. The script can be run by make test.

Write a testing.sh bash script that can be run by make test. This script must include good comments describing your tests. For best results, make test should run bash -v testing.sh.

 

Verify correct behavior by studying the output, and by sampling the files created in the respective pageDirectories.


To test, simply `make test &> testing.out`


